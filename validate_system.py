#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de validation compl√®te du syst√®me CHNeoWave

Ce script effectue une validation compl√®te du syst√®me :
- V√©rification de l'environnement
- Tests de tous les modules
- Validation de l'int√©gration
- Tests de performance
- G√©n√©ration de rapports

Usage:
    python validate_system.py [--quick] [--no-performance] [--output-dir DIR]
"""

import sys
import os
import argparse
import subprocess
import json
import time
from pathlib import Path
from datetime import datetime
import tempfile
import shutil

# Ajouter le r√©pertoire src au PYTHONPATH
project_root = Path(__file__).parent
src_path = project_root / "src"
sys.path.insert(0, str(src_path))

class SystemValidator:
    """Validateur syst√®me pour CHNeoWave"""
    
    def __init__(self, output_dir=None, quick_mode=False, skip_performance=False):
        self.project_root = project_root
        self.output_dir = Path(output_dir) if output_dir else project_root / "validation_reports"
        self.quick_mode = quick_mode
        self.skip_performance = skip_performance
        
        # Cr√©er le r√©pertoire de sortie
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialiser le rapport de validation
        self.validation_report = {
            "timestamp": datetime.now().isoformat(),
            "system_info": {},
            "environment_check": {},
            "module_tests": {},
            "integration_tests": {},
            "performance_tests": {},
            "overall_status": "UNKNOWN",
            "recommendations": []
        }
        
    def log(self, message, level="INFO"):
        """Logger avec timestamp"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        prefix = {
            "INFO": "‚ÑπÔ∏è",
            "SUCCESS": "‚úÖ",
            "WARNING": "‚ö†Ô∏è",
            "ERROR": "‚ùå",
            "DEBUG": "üîç"
        }.get(level, "üìù")
        
        print(f"[{timestamp}] {prefix} {message}")
        
    def run_command(self, cmd, description="", capture_output=True):
        """Ex√©cuter une commande et retourner le r√©sultat"""
        self.log(f"Ex√©cution: {description or ' '.join(cmd)}")
        
        try:
            result = subprocess.run(
                cmd, 
                capture_output=capture_output, 
                text=True, 
                cwd=self.project_root,
                timeout=300  # 5 minutes max
            )
            
            return {
                "success": result.returncode == 0,
                "returncode": result.returncode,
                "stdout": result.stdout if capture_output else "",
                "stderr": result.stderr if capture_output else "",
                "command": " ".join(cmd)
            }
            
        except subprocess.TimeoutExpired:
            self.log(f"Timeout lors de l'ex√©cution de: {' '.join(cmd)}", "ERROR")
            return {
                "success": False,
                "returncode": -1,
                "stdout": "",
                "stderr": "Timeout expired",
                "command": " ".join(cmd)
            }
        except Exception as e:
            self.log(f"Erreur lors de l'ex√©cution: {e}", "ERROR")
            return {
                "success": False,
                "returncode": -1,
                "stdout": "",
                "stderr": str(e),
                "command": " ".join(cmd)
            }
    
    def check_system_info(self):
        """Collecter les informations syst√®me"""
        self.log("Collecte des informations syst√®me", "INFO")
        
        try:
            import platform
            import psutil
            
            self.validation_report["system_info"] = {
                "platform": platform.platform(),
                "python_version": platform.python_version(),
                "architecture": platform.architecture()[0],
                "processor": platform.processor(),
                "cpu_count": psutil.cpu_count(),
                "memory_total_gb": round(psutil.virtual_memory().total / (1024**3), 2),
                "disk_free_gb": round(psutil.disk_usage('.').free / (1024**3), 2)
            }
            
            self.log(f"Syst√®me: {self.validation_report['system_info']['platform']}", "SUCCESS")
            self.log(f"Python: {self.validation_report['system_info']['python_version']}", "SUCCESS")
            self.log(f"M√©moire: {self.validation_report['system_info']['memory_total_gb']} GB", "SUCCESS")
            
        except Exception as e:
            self.log(f"Erreur collecte syst√®me: {e}", "ERROR")
            self.validation_report["system_info"]["error"] = str(e)
    
    def check_environment(self):
        """V√©rifier l'environnement de d√©veloppement"""
        self.log("V√©rification de l'environnement", "INFO")
        
        env_check = {
            "python_path": sys.executable,
            "working_directory": str(self.project_root),
            "src_directory_exists": (self.project_root / "src").exists(),
            "tests_directory_exists": (self.project_root / "tests").exists(),
            "required_packages": {},
            "optional_packages": {}
        }
        
        # V√©rifier les packages requis
        required_packages = [
            'pytest', 'pytest-cov', 'pytest-timeout', 'psutil', 
            'numpy', 'scipy', 'matplotlib', 'PyQt6'
        ]
        
        for package in required_packages:
            try:
                if package == 'PyQt6':
                    import PyQt6
                    env_check["required_packages"][package] = "OK"
                else:
                    __import__(package.replace('-', '_'))
                    env_check["required_packages"][package] = "OK"
            except ImportError:
                env_check["required_packages"][package] = "MISSING"
                self.log(f"Package manquant: {package}", "WARNING")
        
        # V√©rifier les packages optionnels
        optional_packages = ['black', 'flake8', 'mypy', 'sphinx']
        
        for package in optional_packages:
            try:
                __import__(package.replace('-', '_'))
                env_check["optional_packages"][package] = "OK"
            except ImportError:
                env_check["optional_packages"][package] = "MISSING"
        
        self.validation_report["environment_check"] = env_check
        
        # V√©rifier si l'environnement est valide
        missing_required = [pkg for pkg, status in env_check["required_packages"].items() if status == "MISSING"]
        
        if missing_required:
            self.log(f"Packages requis manquants: {', '.join(missing_required)}", "ERROR")
            return False
        else:
            self.log("Environnement valide", "SUCCESS")
            return True
    
    def test_module_imports(self):
        """Tester l'importation de tous les modules"""
        self.log("Test d'importation des modules", "INFO")
        
        modules_to_test = [
            'hrneowave.core.validators',
            'hrneowave.core.error_handler',
            'hrneowave.core.performance_monitor',
            'hrneowave.gui.controllers.main_controller',
            'hrneowave.gui.views.welcome_view'
        ]
        
        import_results = {}
        
        for module in modules_to_test:
            try:
                __import__(module)
                import_results[module] = "SUCCESS"
                self.log(f"Import r√©ussi: {module}", "SUCCESS")
            except Exception as e:
                import_results[module] = f"ERROR: {str(e)}"
                self.log(f"√âchec import {module}: {e}", "ERROR")
        
        self.validation_report["module_tests"]["imports"] = import_results
        
        failed_imports = [mod for mod, status in import_results.items() if status.startswith("ERROR")]
        return len(failed_imports) == 0
    
    def run_unit_tests(self):
        """Ex√©cuter les tests unitaires"""
        self.log("Ex√©cution des tests unitaires", "INFO")
        
        if self.quick_mode:
            cmd = [
                sys.executable, '-m', 'pytest',
                'tests/',
                '-m', 'not slow and not integration and not performance',
                '--tb=short',
                '--disable-warnings',
                '-q'
            ]
        else:
            cmd = [
                sys.executable, '-m', 'pytest',
                'tests/',
                '-m', 'not integration and not performance',
                '--tb=short',
                '--disable-warnings'
            ]
        
        result = self.run_command(cmd, "Tests unitaires")
        self.validation_report["module_tests"]["unit_tests"] = result
        
        if result["success"]:
            self.log("Tests unitaires r√©ussis", "SUCCESS")
        else:
            self.log("√âchec des tests unitaires", "ERROR")
        
        return result["success"]
    
    def run_integration_tests(self):
        """Ex√©cuter les tests d'int√©gration"""
        self.log("Ex√©cution des tests d'int√©gration", "INFO")
        
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/',
            '-m', 'integration',
            '--tb=short',
            '-v'
        ]
        
        result = self.run_command(cmd, "Tests d'int√©gration")
        self.validation_report["integration_tests"] = result
        
        if result["success"]:
            self.log("Tests d'int√©gration r√©ussis", "SUCCESS")
        else:
            self.log("√âchec des tests d'int√©gration", "WARNING")
        
        return result["success"]
    
    def run_performance_tests(self):
        """Ex√©cuter les tests de performance"""
        if self.skip_performance:
            self.log("Tests de performance ignor√©s", "INFO")
            return True
            
        self.log("Ex√©cution des tests de performance", "INFO")
        
        cmd = [
            sys.executable, '-m', 'pytest',
            'tests/',
            '-m', 'performance',
            '--tb=short',
            '--timeout=120'
        ]
        
        result = self.run_command(cmd, "Tests de performance")
        self.validation_report["performance_tests"] = result
        
        if result["success"]:
            self.log("Tests de performance r√©ussis", "SUCCESS")
        else:
            self.log("√âchec des tests de performance", "WARNING")
        
        return result["success"]
    
    def test_error_handling(self):
        """Tester le syst√®me de gestion d'erreurs"""
        self.log("Test du syst√®me de gestion d'erreurs", "INFO")
        
        try:
            from hrneowave.core.error_handler import get_error_handler, ErrorCategory, CHNeoWaveError
            
            # Test basique du gestionnaire d'erreurs
            handler = get_error_handler()
            
            # Cr√©er une erreur de test
            from hrneowave.core.error_handler import ErrorContext
            
            context = ErrorContext(
                component="SystemValidator",
                operation="test_error_handling",
                category=ErrorCategory.SYSTEM
            )
            
            test_error = CHNeoWaveError(
                message="Test de validation syst√®me",
                context=context
            )
            
            # Logger l'erreur
            handler.handle_error(test_error)
            
            # V√©rifier que l'erreur a √©t√© enregistr√©e
            if len(handler.error_history) > 0:
                self.log("Gestionnaire d'erreurs fonctionnel", "SUCCESS")
                
                # Nettoyer l'erreur de test
                handler.clear_error_history()
                return True
            else:
                self.log("Gestionnaire d'erreurs non fonctionnel", "ERROR")
                return False
                
        except Exception as e:
            self.log(f"Erreur lors du test de gestion d'erreurs: {e}", "ERROR")
            return False
    
    def test_performance_monitoring(self):
        """Tester le syst√®me de monitoring de performance"""
        if self.skip_performance:
            return True
            
        self.log("Test du syst√®me de monitoring", "INFO")
        
        try:
            from hrneowave.core.performance_monitor import get_performance_monitor
            
            # Test basique du moniteur
            monitor = get_performance_monitor()
            
            # Collecter des m√©triques
            metrics = monitor.get_current_metrics()
            
            if metrics and hasattr(metrics, 'cpu_percent'):
                self.log(f"Monitoring fonctionnel (CPU: {metrics.cpu_percent:.1f}%)", "SUCCESS")
                return True
            else:
                self.log("Monitoring non fonctionnel", "ERROR")
                return False
                
        except Exception as e:
            self.log(f"Erreur lors du test de monitoring: {e}", "ERROR")
            return False
    
    def generate_recommendations(self):
        """G√©n√©rer des recommandations bas√©es sur les r√©sultats"""
        recommendations = []
        
        # V√©rifier l'environnement
        env_check = self.validation_report.get("environment_check", {})
        missing_required = [pkg for pkg, status in env_check.get("required_packages", {}).items() if status == "MISSING"]
        
        if missing_required:
            recommendations.append({
                "type": "CRITICAL",
                "message": f"Installer les packages requis: {', '.join(missing_required)}",
                "action": f"pip install {' '.join(missing_required)}"
            })
        
        missing_optional = [pkg for pkg, status in env_check.get("optional_packages", {}).items() if status == "MISSING"]
        if missing_optional:
            recommendations.append({
                "type": "SUGGESTION",
                "message": f"Installer les packages optionnels pour le d√©veloppement: {', '.join(missing_optional)}",
                "action": f"pip install {' '.join(missing_optional)}"
            })
        
        # V√©rifier les tests
        if not self.validation_report.get("module_tests", {}).get("unit_tests", {}).get("success", False):
            recommendations.append({
                "type": "CRITICAL",
                "message": "Corriger les √©checs de tests unitaires",
                "action": "Examiner les logs de tests et corriger les erreurs"
            })
        
        if not self.validation_report.get("integration_tests", {}).get("success", False):
            recommendations.append({
                "type": "WARNING",
                "message": "Corriger les √©checs de tests d'int√©gration",
                "action": "V√©rifier la compatibilit√© entre les modules"
            })
        
        # Recommandations de performance
        system_info = self.validation_report.get("system_info", {})
        if system_info.get("memory_total_gb", 0) < 4:
            recommendations.append({
                "type": "WARNING",
                "message": "M√©moire syst√®me faible (< 4GB)",
                "action": "Consid√©rer l'ajout de m√©moire pour de meilleures performances"
            })
        
        self.validation_report["recommendations"] = recommendations
        return recommendations
    
    def determine_overall_status(self):
        """D√©terminer le statut global du syst√®me"""
        # V√©rifier les composants critiques
        critical_checks = [
            self.validation_report.get("environment_check", {}).get("src_directory_exists", False),
            self.validation_report.get("environment_check", {}).get("tests_directory_exists", False),
            len([pkg for pkg, status in self.validation_report.get("environment_check", {}).get("required_packages", {}).items() if status == "MISSING"]) == 0,
            self.validation_report.get("module_tests", {}).get("unit_tests", {}).get("success", False)
        ]
        
        if all(critical_checks):
            self.validation_report["overall_status"] = "HEALTHY"
        elif any(critical_checks):
            self.validation_report["overall_status"] = "WARNING"
        else:
            self.validation_report["overall_status"] = "CRITICAL"
    
    def save_report(self):
        """Sauvegarder le rapport de validation"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.output_dir / f"validation_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.validation_report, f, indent=2, ensure_ascii=False)
        
        self.log(f"Rapport sauvegard√©: {report_file}", "SUCCESS")
        
        # Cr√©er aussi un rapport texte lisible
        text_report = self.output_dir / f"validation_summary_{timestamp}.txt"
        self.generate_text_report(text_report)
        
        return report_file
    
    def generate_text_report(self, output_file):
        """G√©n√©rer un rapport texte lisible"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("CHNeoWave - Rapport de Validation Syst√®me\n")
            f.write("=" * 50 + "\n\n")
            
            f.write(f"Date: {self.validation_report['timestamp']}\n")
            f.write(f"Statut global: {self.validation_report['overall_status']}\n\n")
            
            # Informations syst√®me
            f.write("INFORMATIONS SYST√àME\n")
            f.write("-" * 20 + "\n")
            system_info = self.validation_report.get('system_info', {})
            for key, value in system_info.items():
                f.write(f"{key}: {value}\n")
            f.write("\n")
            
            # Environnement
            f.write("ENVIRONNEMENT\n")
            f.write("-" * 13 + "\n")
            env_check = self.validation_report.get('environment_check', {})
            
            f.write("Packages requis:\n")
            for pkg, status in env_check.get('required_packages', {}).items():
                f.write(f"  {pkg}: {status}\n")
            
            f.write("\nPackages optionnels:\n")
            for pkg, status in env_check.get('optional_packages', {}).items():
                f.write(f"  {pkg}: {status}\n")
            f.write("\n")
            
            # Tests
            f.write("R√âSULTATS DES TESTS\n")
            f.write("-" * 18 + "\n")
            
            unit_tests = self.validation_report.get('module_tests', {}).get('unit_tests', {})
            f.write(f"Tests unitaires: {'R√âUSSI' if unit_tests.get('success') else '√âCHEC'}\n")
            
            integration_tests = self.validation_report.get('integration_tests', {})
            f.write(f"Tests d'int√©gration: {'R√âUSSI' if integration_tests.get('success') else '√âCHEC'}\n")
            
            performance_tests = self.validation_report.get('performance_tests', {})
            if performance_tests:
                f.write(f"Tests de performance: {'R√âUSSI' if performance_tests.get('success') else '√âCHEC'}\n")
            f.write("\n")
            
            # Recommandations
            f.write("RECOMMANDATIONS\n")
            f.write("-" * 15 + "\n")
            for rec in self.validation_report.get('recommendations', []):
                f.write(f"[{rec['type']}] {rec['message']}\n")
                f.write(f"  Action: {rec['action']}\n\n")
        
        self.log(f"Rapport texte g√©n√©r√©: {output_file}", "SUCCESS")
    
    def run_validation(self):
        """Ex√©cuter la validation compl√®te"""
        self.log("D√©but de la validation syst√®me CHNeoWave", "INFO")
        start_time = time.time()
        
        try:
            # 1. Informations syst√®me
            self.check_system_info()
            
            # 2. V√©rification environnement
            env_ok = self.check_environment()
            if not env_ok:
                self.log("Environnement invalide, arr√™t de la validation", "ERROR")
                self.validation_report["overall_status"] = "CRITICAL"
                return False
            
            # 3. Test d'importation des modules
            imports_ok = self.test_module_imports()
            if not imports_ok:
                self.log("√âchec d'importation des modules", "ERROR")
            
            # 4. Tests unitaires
            unit_tests_ok = self.run_unit_tests()
            
            # 5. Tests d'int√©gration
            integration_ok = self.run_integration_tests()
            
            # 6. Tests de performance
            performance_ok = self.run_performance_tests()
            
            # 7. Test des syst√®mes critiques
            error_handling_ok = self.test_error_handling()
            monitoring_ok = self.test_performance_monitoring()
            
            # 8. G√©n√©rer recommandations
            self.generate_recommendations()
            
            # 9. D√©terminer le statut global
            self.determine_overall_status()
            
            # 10. Sauvegarder le rapport
            report_file = self.save_report()
            
            # R√©sum√© final
            elapsed_time = time.time() - start_time
            self.log(f"Validation termin√©e en {elapsed_time:.1f}s", "INFO")
            self.log(f"Statut global: {self.validation_report['overall_status']}", 
                    "SUCCESS" if self.validation_report['overall_status'] == "HEALTHY" else "WARNING")
            
            return self.validation_report['overall_status'] in ["HEALTHY", "WARNING"]
            
        except Exception as e:
            self.log(f"Erreur critique lors de la validation: {e}", "ERROR")
            self.validation_report["overall_status"] = "CRITICAL"
            self.validation_report["critical_error"] = str(e)
            return False

def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(
        description="Validation compl√®te du syst√®me CHNeoWave",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--quick', action='store_true', 
                       help='Mode rapide (tests unitaires seulement)')
    parser.add_argument('--no-performance', action='store_true',
                       help='Ignorer les tests de performance')
    parser.add_argument('--output-dir', type=str,
                       help='R√©pertoire de sortie pour les rapports')
    
    args = parser.parse_args()
    
    # Cr√©er le validateur
    validator = SystemValidator(
        output_dir=args.output_dir,
        quick_mode=args.quick,
        skip_performance=args.no_performance
    )
    
    # Ex√©cuter la validation
    success = validator.run_validation()
    
    # Code de sortie
    sys.exit(0 if success else 1)

if __name__ == '__main__':
    main()